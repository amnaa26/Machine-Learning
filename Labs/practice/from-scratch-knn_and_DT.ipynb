{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c392ea8-fa54-454f-a8ac-5b42f4a74b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b80e7c9-fabd-47a2-af36-792b73f4e076",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating sample data\n",
    "X = np.random.rand(10, 4) * 10; #10 samples with 4 features\n",
    "y = np.random.randint(0, 2, size=10) #2 labels of 10 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "75e8d3b9-19b7-4f59-97d9-6adfdf340514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(a, b):\n",
    "    return np.sqrt(np.sum((a-b)**2))\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        self.x_train = x\n",
    "        self.y_train = y\n",
    "\n",
    "    def predict(self, x):\n",
    "        #compute distance\n",
    "        distances = [euclidean_distance(x, x_train) for x_train in self.x_train]\n",
    "\n",
    "        #get k neighbors\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "\n",
    "        # majority vote\n",
    "        k_neigbors_label = [self.y_train[i] for i in k_indices]\n",
    "        most_common = Counter(k_neigbors_label).most_common(1)[0][0]\n",
    "        return most_common\n",
    "\n",
    "    def _predict(self, test):\n",
    "        return [self.predict(x) for x in test]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7c252cf7-5bab-4cbb-bdda-eca0644b678c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Test Split Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, shuffle=True, random_state=42)\n",
    "knn = KNN()\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn._predict(X_test)\n",
    "\n",
    "accuracy = sum(1 for a, b in zip(y_pred, y_test) if a == b) / len(y_test)\n",
    "print(\"Train-Test Split Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "28517357-41ac-4b51-bb3e-d1d47da8d59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOCV Accuracy: 0.9600000000000001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn._predict(X_test)\n",
    "\n",
    "    accuracies.append(int(y_pred[0] == y_test[0]))\n",
    "\n",
    "print(\"LOOCV Accuracy:\", sum(accuracies) / len(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "25fac98e-e466-4b0f-b279-ed4adfdb6ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-Fold Cross Validation Accuracy: 0.9602083333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn._predict(X_test)\n",
    "\n",
    "    acc = sum(1 for a, b in zip(y_pred, y_test) if a == b) / len(y_test)\n",
    "    accuracies.append(acc)\n",
    "\n",
    "print(\"5-Fold Cross Validation Accuracy:\", sum(accuracies) / len(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e8aec1-7faf-4353-88ba-c5f45ab9c430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b42c7290-0bdf-4b5c-94bb-18ca96c2cad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(feature):\n",
    "    counts = Counter(feature) #frequency of each label\n",
    "    total = len(feature)\n",
    "    entropy_ = 0.0\n",
    "    for c in counts.values():\n",
    "        p = c/total\n",
    "        if p > 0:\n",
    "            entropy_ -= p * np.log2(p)\n",
    "    return entropy_\n",
    "\n",
    "'''\n",
    "def info_gain(X_column, y, split_value=None): #X_column are the values of a feature, y is the label of each value, and split_value defines whether the feature is categorical or numerical\n",
    "    parent_entropy = entropy(y)\n",
    "    n = len(y)\n",
    "    total_entropy = 0.0\n",
    "    if split_value is None: #categorical feature\n",
    "        values = set(X_column) #it gives us the unique values eg feature has_job=['yes', 'no', 'no', 'yes', 'yes'] ---> with set: values=['yes', 'no'], removes duplicates\n",
    "        for v in values:\n",
    "            y_subset = [y[i] for i in range(len(y)) if X_column[i] == v] \n",
    "            total_entropy += (len(y_subset)/n) * entropy(y_subset)\n",
    "    else: #binary split when split_value is given\n",
    "        if isinstance(split_value, (int, float)): #if true then numerical feature\n",
    "            left_y = [y[i] for i in range(n) if X_column[i] <= split_value]\n",
    "            right_y = [y[i] for i in range(n) if X_column[i] > split_value]\n",
    "        else: #catagorical value\n",
    "            left_y = [y[i] for i in range(n) if X_column[i] == split_value]\n",
    "            right_y = [y[i] for i in range(n) if X_column[i] != split_value]\n",
    "\n",
    "        #if one side is empty\n",
    "        if len(left_y) == 0 or len(right_y) == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        total_entropy += ((len(left_y)/n) * entropy(left_y)) + ((len(right_y)/n) * entropy(right_y))\n",
    "\n",
    "    return parent_entropy - total_entropy\n",
    "'''\n",
    "\n",
    "def info_gain(X_column, y, split_value):\n",
    "    parent_entropy = entropy(y)\n",
    "    n = len(y)\n",
    "\n",
    "    if isinstance(split_value, (int, float)):\n",
    "        # numeric split\n",
    "        left_y = [y[i] for i in range(n) if X_column[i] <= split_value]\n",
    "        right_y = [y[i] for i in range(n) if X_column[i] > split_value]\n",
    "    else:\n",
    "        # categorical split\n",
    "        left_y = [y[i] for i in range(n) if X_column[i] == split_value]\n",
    "        right_y = [y[i] for i in range(n) if X_column[i] != split_value]\n",
    "\n",
    "    # if one branch is empty, no information gain\n",
    "    if len(left_y) == 0 or len(right_y) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    weighted_entropy = (len(left_y)/n) * entropy(left_y) + (len(right_y)/n) * entropy(right_y)\n",
    "    return parent_entropy - weighted_entropy\n",
    "\n",
    "\n",
    "#  -----Decision Tree-----\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, right=None, left=None, *, value=None):\n",
    "        self.right = right\n",
    "        self.left = left\n",
    "        self.value = value\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return self.value is not None\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=5):\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        self.root = self.grow_tree(x, y)\n",
    "\n",
    "    '''\n",
    "    def grow_tree(self, x, y, depth=0): #recursive function\n",
    "        #if empty dataset\n",
    "        if len(x) == 0:\n",
    "            return Node(value=None)\n",
    "        \n",
    "        #stopping condition\n",
    "        if len(set(y)) == 1: #all labels are the same i.e pure node, one unique label\n",
    "            return Node(value=y[0]) \n",
    "\n",
    "        num_features = len(x[0]) if len(x) > 0 else 0\n",
    "        if depth >= self.max_depth or num_features == 0: #if we reached max_depth or no more features to split\n",
    "            return Node(value=Counter(y).most_common(1)[0][0]) #returning leaf node with the majority class\n",
    "\n",
    "        # ---- choosing best feature to split -----\n",
    "        best_gain = -1.0\n",
    "        split_index, split_threshold = None, None\n",
    "\n",
    "         #split_index --> stores the index of the feature ;feature on which split has to be done eg:has job splits into yes and no\n",
    "         #Example: if your data x = [[Age, Color, Student], …], and the best split is on \"Color\", then split_index = 1 (the column number).\n",
    "         #split_threshold --> stores actual value of the feature on which we are splitting\n",
    "         #Example: If the feature is numeric (e.g., Age=30), then threshold is that number → split into ≤30 vs >30.\n",
    "                   If the feature is categorical (e.g., Color=\"Red\"), then threshold is that category → split into \"Red\" vs \"Not Red\".\n",
    "\n",
    "        for feature in range(num_features):\n",
    "            #col = [row[feature] for row in x] --> list of features\n",
    "            #value = set(col) --> values of each features\n",
    "            values = set(row[feature] for row in x) #equivalent to value=sorted(set(col)\n",
    "            for val in values:\n",
    "                gain = info_gain([row[feature] for row in x], y, val if isinstance(val, (int, float)) else val) #check\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_index = feature\n",
    "                    split_threshold = val\n",
    "                    \n",
    "        if best_gain <= 0:\n",
    "            return Node(value=Counter(y).most_common(1)[0][0])\n",
    "\n",
    "        #splitting dataset\n",
    "        if isinstance(split_threshold, (int, float)):\n",
    "            left_index = [i for i in range(len(x)) if x[i][split_index] <= split_threshold]\n",
    "            right_index = [i for i in range(len(x)) if x[i][split_index] > split_threshold]\n",
    "\n",
    "        else: #categorical\n",
    "            left_index = [i for i in range(len(x)) if x[i][split_index] == split_threshold]\n",
    "            right_index = [i for i in range(len(x)) if x[i][split_index] != split_threshold]\n",
    "\n",
    "        left = self.grow_tree([x[i] for i in left_index], [y[i] for i in left_index], depth+1)\n",
    "        right = self.grow_tree([x[i] for i in right_index], [y[i] for i in right_index], depth+1)\n",
    "\n",
    "        #this is equivalent to the above left and right recursive calls\n",
    "        #X_left  = [X[i] for i in left_idx]\n",
    "        #y_left  = [y[i] for i in left_idx]\n",
    "        #X_right = [X[i] for i in right_idx]\n",
    "        #y_right = [y[i] for i in right_idx]\n",
    "\n",
    "        # recursive calls\n",
    "        #left_child  = self.grow_tree(X_left, y_left, depth + 1)\n",
    "        #right_child = self.grow_tree(X_right, y_right, depth + 1)\n",
    "        \n",
    "        return Node(split_index, split_threshold, left, right)\n",
    "\n",
    "        '''\n",
    "\n",
    "\n",
    "    def grow_tree(self, x, y, depth=0):\n",
    "        # base cases\n",
    "        if len(x) == 0:\n",
    "            return Node(value=None)\n",
    "        if len(set(y)) == 1:\n",
    "            return Node(value=y[0])\n",
    "        if depth >= self.max_depth:\n",
    "            return Node(value=Counter(y).most_common(1)[0][0])\n",
    "    \n",
    "        num_features = len(x[0])\n",
    "        best_gain = -1.0\n",
    "        split_index, split_threshold = None, None\n",
    "    \n",
    "        # loop through features\n",
    "        for feature in range(num_features):\n",
    "            values = sorted(set(row[feature] for row in x))\n",
    "    \n",
    "            # decide candidate thresholds\n",
    "            if all(isinstance(v, (int, float)) for v in values):\n",
    "                # numeric -> use midpoints\n",
    "                candidates = [(values[i] + values[i+1]) / 2 for i in range(len(values)-1)]\n",
    "            else:\n",
    "                # categorical -> use unique values\n",
    "                candidates = values\n",
    "    \n",
    "            # test each candidate\n",
    "            for val in candidates:\n",
    "                gain = info_gain([row[feature] for row in x], y, val)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_index = feature\n",
    "                    split_threshold = val\n",
    "    \n",
    "        # if no useful split, make leaf\n",
    "        if best_gain <= 0:\n",
    "            return Node(value=Counter(y).most_common(1)[0][0])\n",
    "    \n",
    "        # split dataset\n",
    "        if isinstance(split_threshold, (int, float)):\n",
    "            left_index = [i for i in range(len(x)) if x[i][split_index] <= split_threshold]\n",
    "            right_index = [i for i in range(len(x)) if x[i][split_index] > split_threshold]\n",
    "        else:\n",
    "            left_index = [i for i in range(len(x)) if x[i][split_index] == split_threshold]\n",
    "            right_index = [i for i in range(len(x)) if x[i][split_index] != split_threshold]\n",
    "    \n",
    "        left = self.grow_tree([x[i] for i in left_index], [y[i] for i in left_index], depth+1)\n",
    "        right = self.grow_tree([x[i] for i in right_index], [y[i] for i in right_index], depth+1)\n",
    "    \n",
    "        return Node(feature=split_index, threshold=split_threshold, left=left, right=right)\n",
    "\n",
    "\n",
    "    def predict_one(self, x):\n",
    "            node = self.root\n",
    "            while not node.is_leaf():\n",
    "                if isinstance(node.threshold, (int, float)):\n",
    "                    if x[node.feature] <= node.threshold:\n",
    "                        node = node.left\n",
    "                    else:\n",
    "                        node = node.right\n",
    "                else: #categorical\n",
    "                    if x[node.feature] == node.threshold:\n",
    "                        node = node.left\n",
    "                    else:\n",
    "                        node = node.right\n",
    "            return node.value\n",
    "\n",
    "    def predict(self, X):\n",
    "            return [self.predict_one(x) for x in X]\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "241eb3d6-59f7-4088-99ae-c9032688fd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yes', 'No']\n"
     ]
    }
   ],
   "source": [
    "# Small dataset: [Age, Has_Job, Owns_House] -> Buy?\n",
    "X = [\n",
    "    [25, \"No\", \"No\"],\n",
    "    [30, \"Yes\", \"No\"],\n",
    "    [40, \"Yes\", \"Yes\"],\n",
    "    [35, \"Yes\", \"Yes\"],\n",
    "    [22, \"No\", \"No\"]\n",
    "]\n",
    "y = [\"No\", \"Yes\", \"Yes\", \"Yes\", \"No\"]\n",
    "\n",
    "tree = DecisionTree(max_depth=3)\n",
    "tree.fit(X, y)\n",
    "\n",
    "print(tree.predict([[28, \"Yes\", \"No\"], [23, \"No\", \"No\"]]))\n",
    "# Output: ['Yes', 'No']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "edca0415-370c-4b4a-8874-b4f19f13d6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Actual: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, LeaveOneOut\n",
    "import pandas as pd\n",
    "\n",
    "# load iris\n",
    "iris = load_iris()\n",
    "X = iris.data        # numeric features\n",
    "y = iris.target      # labels (0,1,2)\n",
    "\n",
    "# convert to list of lists (since your tree uses Python lists)\n",
    "X = X.tolist()\n",
    "y = y.tolist()\n",
    "\n",
    "# train your custom tree\n",
    "tree = DecisionTree(max_depth=3)\n",
    "tree.fit(X, y)\n",
    "\n",
    "# predict a few samples\n",
    "print(tree.predict(X[30:]))\n",
    "print(\"Actual:\", y[30:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d504131b-12c1-409d-b910-6c880d018dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Test Split Accuracy: 0.9666666666666667\n",
      "[1, 0, 2, 1, 2, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0]\n",
      "Actual: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "iris = load_iris()\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "tree = DecisionTree(max_depth=3)\n",
    "tree.fit(x_train.tolist(), y_train.tolist())\n",
    "\n",
    "#predict\n",
    "y_pred = tree.predict(x_test.tolist())\n",
    "\n",
    "accuracy = sum(1 for a, b in zip(y_pred, y_test) if a == b) / len(y_test)\n",
    "print(\"Train-Test Split Accuracy:\", accuracy)\n",
    "\n",
    "print(y_pred)\n",
    "print(\"Actual:\", y_test[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "739030ec-0ecd-4150-8202-facae13442ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOCV Accuracy: 0.9533333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "accuracies = []\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    tree = DecisionTree(max_depth=3)\n",
    "    tree.fit(X_train.tolist(), Y_train.tolist())\n",
    "    y_pred = tree.predict(X_test.tolist())\n",
    "    accuracies.append(int(y_pred[0] == Y_test[0]))\n",
    "\n",
    "print(\"LOOCV Accuracy:\", sum(accuracies) / len(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "873b9a79-4fca-4223-9383-beb37241c98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-Fold Cross Validation Accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "accuracies = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    tree = DecisionTree(max_depth=3)\n",
    "    tree.fit(X_train.tolist(), y_train.tolist())\n",
    "\n",
    "    y_pred = tree.predict(X_test.tolist())\n",
    "    \n",
    "    acc = sum(1 for a, b in zip(y_pred, y_test) if a == b) / len(y_test)\n",
    "    accuracies.append(acc)\n",
    "\n",
    "print(\"5-Fold Cross Validation Accuracy:\", sum(accuracies) / len(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4698fac7-62d3-4f5e-8a44-b23923fb0d52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
