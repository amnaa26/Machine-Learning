{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c392ea8-fa54-454f-a8ac-5b42f4a74b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b80e7c9-fabd-47a2-af36-792b73f4e076",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating sample data\n",
    "X = np.random.rand(10, 4) * 10; #10 samples with 4 features\n",
    "y = np.random.randint(0, 2, size=10) #2 labels of 10 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "75e8d3b9-19b7-4f59-97d9-6adfdf340514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(a, b):\n",
    "    return np.sqrt(np.sum((a-b)**2))\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        self.x_train = x\n",
    "        self.y_train = y\n",
    "\n",
    "    def predict(self, x):\n",
    "        #compute distance\n",
    "        distances = [euclidean_distance(x, x_train) for x_train in self.x_train]\n",
    "\n",
    "        #get k neighbors\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "\n",
    "        # majority vote\n",
    "        k_neigbors_label = [self.y_train[i] for i in k_indices]\n",
    "        most_common = Counter(k_neigbors_label).most_common(1)[0][0]\n",
    "        return most_common\n",
    "\n",
    "    def _predict(self, test):\n",
    "        return [self.predict(x) for x in test]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7c252cf7-5bab-4cbb-bdda-eca0644b678c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Test Split Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, shuffle=True, random_state=42)\n",
    "knn = KNN()\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn._predict(X_test)\n",
    "\n",
    "accuracy = sum(1 for a, b in zip(y_pred, y_test) if a == b) / len(y_test)\n",
    "print(\"Train-Test Split Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "28517357-41ac-4b51-bb3e-d1d47da8d59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOCV Accuracy: 0.9600000000000001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn._predict(X_test)\n",
    "\n",
    "    accuracies.append(int(y_pred[0] == y_test[0]))\n",
    "\n",
    "print(\"LOOCV Accuracy:\", sum(accuracies) / len(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "25fac98e-e466-4b0f-b279-ed4adfdb6ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-Fold Cross Validation Accuracy: 0.9602083333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn._predict(X_test)\n",
    "\n",
    "    acc = sum(1 for a, b in zip(y_pred, y_test) if a == b) / len(y_test)\n",
    "    accuracies.append(acc)\n",
    "\n",
    "print(\"5-Fold Cross Validation Accuracy:\", sum(accuracies) / len(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e8aec1-7faf-4353-88ba-c5f45ab9c430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b42c7290-0bdf-4b5c-94bb-18ca96c2cad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(feature):\n",
    "    counts = Counter(feature) #frequency of each label\n",
    "    total = len(feature)\n",
    "    entropy_ = 0.0\n",
    "    for c in counts.values():\n",
    "        p = c/total\n",
    "        if p > 0:\n",
    "            entropy_ -= p * np.log2(p)\n",
    "    return entropy_\n",
    "\n",
    "'''\n",
    "def info_gain(X_column, y, split_value=None): #X_column are the values of a feature, y is the label of each value, and split_value defines whether the feature is categorical or numerical\n",
    "    parent_entropy = entropy(y)\n",
    "    n = len(y)\n",
    "    total_entropy = 0.0\n",
    "    if split_value is None: #categorical feature\n",
    "        values = set(X_column) #it gives us the unique values eg feature has_job=['yes', 'no', 'no', 'yes', 'yes'] ---> with set: values=['yes', 'no'], removes duplicates\n",
    "        for v in values:\n",
    "            y_subset = [y[i] for i in range(len(y)) if X_column[i] == v] \n",
    "            total_entropy += (len(y_subset)/n) * entropy(y_subset)\n",
    "    else: #binary split when split_value is given\n",
    "        if isinstance(split_value, (int, float)): #if true then numerical feature\n",
    "            left_y = [y[i] for i in range(n) if X_column[i] <= split_value]\n",
    "            right_y = [y[i] for i in range(n) if X_column[i] > split_value]\n",
    "        else: #catagorical value\n",
    "            left_y = [y[i] for i in range(n) if X_column[i] == split_value]\n",
    "            right_y = [y[i] for i in range(n) if X_column[i] != split_value]\n",
    "\n",
    "        #if one side is empty\n",
    "        if len(left_y) == 0 or len(right_y) == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        total_entropy += ((len(left_y)/n) * entropy(left_y)) + ((len(right_y)/n) * entropy(right_y))\n",
    "\n",
    "    return parent_entropy - total_entropy\n",
    "'''\n",
    "\n",
    "def info_gain(X_column, y, split_value):\n",
    "    parent_entropy = entropy(y)\n",
    "    n = len(y)\n",
    "\n",
    "    if isinstance(split_value, (int, float)):\n",
    "        # numeric split\n",
    "        left_y = [y[i] for i in range(n) if X_column[i] <= split_value]\n",
    "        right_y = [y[i] for i in range(n) if X_column[i] > split_value]\n",
    "    else:\n",
    "        # categorical split\n",
    "        left_y = [y[i] for i in range(n) if X_column[i] == split_value]\n",
    "        right_y = [y[i] for i in range(n) if X_column[i] != split_value]\n",
    "\n",
    "    # if one branch is empty, no information gain\n",
    "    if len(left_y) == 0 or len(right_y) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    weighted_entropy = (len(left_y)/n) * entropy(left_y) + (len(right_y)/n) * entropy(right_y)\n",
    "    return parent_entropy - weighted_entropy\n",
    "\n",
    "\n",
    "#  -----Decision Tree-----\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, right=None, left=None, *, value=None):\n",
    "        self.right = right\n",
    "        self.left = left\n",
    "        self.value = value\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return self.value is not None\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=5):\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        self.root = self.grow_tree(x, y)\n",
    "\n",
    "    '''\n",
    "    def grow_tree(self, x, y, depth=0): #recursive function\n",
    "        #if empty dataset\n",
    "        if len(x) == 0:\n",
    "            return Node(value=None)\n",
    "        \n",
    "        #stopping condition\n",
    "        if len(set(y)) == 1: #all labels are the same i.e pure node, one unique label\n",
    "            return Node(value=y[0]) \n",
    "\n",
    "        num_features = len(x[0]) if len(x) > 0 else 0\n",
    "        if depth >= self.max_depth or num_features == 0: #if we reached max_depth or no more features to split\n",
    "            return Node(value=Counter(y).most_common(1)[0][0]) #returning leaf node with the majority class\n",
    "\n",
    "        # ---- choosing best feature to split -----\n",
    "        best_gain = -1.0\n",
    "        split_index, split_threshold = None, None\n",
    "\n",
    "         #split_index --> stores the index of the feature ;feature on which split has to be done eg:has job splits into yes and no\n",
    "         #Example: if your data x = [[Age, Color, Student], …], and the best split is on \"Color\", then split_index = 1 (the column number).\n",
    "         #split_threshold --> stores actual value of the feature on which we are splitting\n",
    "         #Example: If the feature is numeric (e.g., Age=30), then threshold is that number → split into ≤30 vs >30.\n",
    "                   If the feature is categorical (e.g., Color=\"Red\"), then threshold is that category → split into \"Red\" vs \"Not Red\".\n",
    "\n",
    "        for feature in range(num_features):\n",
    "            #col = [row[feature] for row in x] --> list of features\n",
    "            #value = set(col) --> values of each features\n",
    "            values = set(row[feature] for row in x) #equivalent to value=sorted(set(col)\n",
    "            for val in values:\n",
    "                gain = info_gain([row[feature] for row in x], y, val if isinstance(val, (int, float)) else val) #check\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_index = feature\n",
    "                    split_threshold = val\n",
    "                    \n",
    "        if best_gain <= 0:\n",
    "            return Node(value=Counter(y).most_common(1)[0][0])\n",
    "\n",
    "        #splitting dataset\n",
    "        if isinstance(split_threshold, (int, float)):\n",
    "            left_index = [i for i in range(len(x)) if x[i][split_index] <= split_threshold]\n",
    "            right_index = [i for i in range(len(x)) if x[i][split_index] > split_threshold]\n",
    "\n",
    "        else: #categorical\n",
    "            left_index = [i for i in range(len(x)) if x[i][split_index] == split_threshold]\n",
    "            right_index = [i for i in range(len(x)) if x[i][split_index] != split_threshold]\n",
    "\n",
    "        left = self.grow_tree([x[i] for i in left_index], [y[i] for i in left_index], depth+1)\n",
    "        right = self.grow_tree([x[i] for i in right_index], [y[i] for i in right_index], depth+1)\n",
    "\n",
    "        #this is equivalent to the above left and right recursive calls\n",
    "        #X_left  = [X[i] for i in left_idx]\n",
    "        #y_left  = [y[i] for i in left_idx]\n",
    "        #X_right = [X[i] for i in right_idx]\n",
    "        #y_right = [y[i] for i in right_idx]\n",
    "\n",
    "        # recursive calls\n",
    "        #left_child  = self.grow_tree(X_left, y_left, depth + 1)\n",
    "        #right_child = self.grow_tree(X_right, y_right, depth + 1)\n",
    "        \n",
    "        return Node(split_index, split_threshold, left, right)\n",
    "\n",
    "        '''\n",
    "\n",
    "\n",
    "    def grow_tree(self, x, y, depth=0):\n",
    "        # base cases\n",
    "        if len(x) == 0:\n",
    "            return Node(value=None)\n",
    "        if len(set(y)) == 1:\n",
    "            return Node(value=y[0])\n",
    "        if depth >= self.max_depth:\n",
    "            return Node(value=Counter(y).most_common(1)[0][0])\n",
    "    \n",
    "        num_features = len(x[0])\n",
    "        best_gain = -1.0\n",
    "        split_index, split_threshold = None, None\n",
    "    \n",
    "        # loop through features\n",
    "        for feature in range(num_features):\n",
    "            values = sorted(set(row[feature] for row in x))\n",
    "    \n",
    "            # decide candidate thresholds\n",
    "            if all(isinstance(v, (int, float)) for v in values):\n",
    "                # numeric -> use midpoints\n",
    "                candidates = [(values[i] + values[i+1]) / 2 for i in range(len(values)-1)]\n",
    "            else:\n",
    "                # categorical -> use unique values\n",
    "                candidates = values\n",
    "    \n",
    "            # test each candidate\n",
    "            for val in candidates:\n",
    "                gain = info_gain([row[feature] for row in x], y, val)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_index = feature\n",
    "                    split_threshold = val\n",
    "    \n",
    "        # if no useful split, make leaf\n",
    "        if best_gain <= 0:\n",
    "            return Node(value=Counter(y).most_common(1)[0][0])\n",
    "    \n",
    "        # split dataset\n",
    "        if isinstance(split_threshold, (int, float)):\n",
    "            left_index = [i for i in range(len(x)) if x[i][split_index] <= split_threshold]\n",
    "            right_index = [i for i in range(len(x)) if x[i][split_index] > split_threshold]\n",
    "        else:\n",
    "            left_index = [i for i in range(len(x)) if x[i][split_index] == split_threshold]\n",
    "            right_index = [i for i in range(len(x)) if x[i][split_index] != split_threshold]\n",
    "    \n",
    "        left = self.grow_tree([x[i] for i in left_index], [y[i] for i in left_index], depth+1)\n",
    "        right = self.grow_tree([x[i] for i in right_index], [y[i] for i in right_index], depth+1)\n",
    "    \n",
    "        return Node(feature=split_index, threshold=split_threshold, left=left, right=right)\n",
    "\n",
    "\n",
    "    def predict_one(self, x):\n",
    "            node = self.root\n",
    "            while not node.is_leaf():\n",
    "                if isinstance(node.threshold, (int, float)):\n",
    "                    if x[node.feature] <= node.threshold:\n",
    "                        node = node.left\n",
    "                    else:\n",
    "                        node = node.right\n",
    "                else: #categorical\n",
    "                    if x[node.feature] == node.threshold:\n",
    "                        node = node.left\n",
    "                    else:\n",
    "                        node = node.right\n",
    "            return node.value\n",
    "\n",
    "    def predict(self, X):\n",
    "            return [self.predict_one(x) for x in X]\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "241eb3d6-59f7-4088-99ae-c9032688fd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yes', 'No']\n"
     ]
    }
   ],
   "source": [
    "# Small dataset: [Age, Has_Job, Owns_House] -> Buy?\n",
    "X = [\n",
    "    [25, \"No\", \"No\"],\n",
    "    [30, \"Yes\", \"No\"],\n",
    "    [40, \"Yes\", \"Yes\"],\n",
    "    [35, \"Yes\", \"Yes\"],\n",
    "    [22, \"No\", \"No\"]\n",
    "]\n",
    "y = [\"No\", \"Yes\", \"Yes\", \"Yes\", \"No\"]\n",
    "\n",
    "tree = DecisionTree(max_depth=3)\n",
    "tree.fit(X, y)\n",
    "\n",
    "print(tree.predict([[28, \"Yes\", \"No\"], [23, \"No\", \"No\"]]))\n",
    "# Output: ['Yes', 'No']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "edca0415-370c-4b4a-8874-b4f19f13d6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Actual: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, LeaveOneOut\n",
    "import pandas as pd\n",
    "\n",
    "# load iris\n",
    "iris = load_iris()\n",
    "X = iris.data        # numeric features\n",
    "y = iris.target      # labels (0,1,2)\n",
    "\n",
    "# convert to list of lists (since your tree uses Python lists)\n",
    "X = X.tolist()\n",
    "y = y.tolist()\n",
    "\n",
    "# train your custom tree\n",
    "tree = DecisionTree(max_depth=3)\n",
    "tree.fit(X, y)\n",
    "\n",
    "# predict a few samples\n",
    "print(tree.predict(X[30:]))\n",
    "print(\"Actual:\", y[30:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d504131b-12c1-409d-b910-6c880d018dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Test Split Accuracy: 0.9666666666666667\n",
      "[1, 0, 2, 1, 2, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0]\n",
      "Actual: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "iris = load_iris()\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "tree = DecisionTree(max_depth=3)\n",
    "tree.fit(x_train.tolist(), y_train.tolist())\n",
    "\n",
    "#predict\n",
    "y_pred = tree.predict(x_test.tolist())\n",
    "\n",
    "accuracy = sum(1 for a, b in zip(y_pred, y_test) if a == b) / len(y_test)\n",
    "print(\"Train-Test Split Accuracy:\", accuracy)\n",
    "\n",
    "print(y_pred)\n",
    "print(\"Actual:\", y_test[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "739030ec-0ecd-4150-8202-facae13442ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOCV Accuracy: 0.9533333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "accuracies = []\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    tree = DecisionTree(max_depth=3)\n",
    "    tree.fit(X_train.tolist(), Y_train.tolist())\n",
    "    y_pred = tree.predict(X_test.tolist())\n",
    "    accuracies.append(int(y_pred[0] == Y_test[0]))\n",
    "\n",
    "print(\"LOOCV Accuracy:\", sum(accuracies) / len(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "873b9a79-4fca-4223-9383-beb37241c98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-Fold Cross Validation Accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "accuracies = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    tree = DecisionTree(max_depth=3)\n",
    "    tree.fit(X_train.tolist(), y_train.tolist())\n",
    "\n",
    "    y_pred = tree.predict(X_test.tolist())\n",
    "    \n",
    "    acc = sum(1 for a, b in zip(y_pred, y_test) if a == b) / len(y_test)\n",
    "    accuracies.append(acc)\n",
    "\n",
    "print(\"5-Fold Cross Validation Accuracy:\", sum(accuracies) / len(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c744ed9d-c493-47f6-b403-397212075199",
   "metadata": {},
   "source": [
    "## Another implementation of DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79819562-a72a-48fd-9bac-74059ff0b646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "        \n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split=2, max_depth=100, n_features=None):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.n_features = n_features\n",
    "        self.root = None\n",
    "        self.feature_types = {}  # Store feature types for prediction\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Convert to numpy arrays and handle mixed data types\n",
    "        X = self._convert_to_array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        # Determine feature types\n",
    "        self._determine_feature_types(X)\n",
    "        \n",
    "        self.n_features = X.shape[1] if not self.n_features else min(X.shape[1], self.n_features)\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    def _convert_to_array(self, X):\n",
    "        \"\"\"Convert input to numpy array while preserving data types\"\"\"\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            # Convert each column separately to handle mixed types\n",
    "            arrays = []\n",
    "            for col in X.columns:\n",
    "                col_data = X[col].values\n",
    "                # Try to convert to numeric, if fails keep as object\n",
    "                try:\n",
    "                    arrays.append(pd.to_numeric(col_data).values.reshape(-1, 1))\n",
    "                except:\n",
    "                    arrays.append(col_data.reshape(-1, 1))\n",
    "            return np.hstack(arrays)\n",
    "        else:\n",
    "            return np.array(X)\n",
    "\n",
    "    def _determine_feature_types(self, X):\n",
    "        \"\"\"Determine if each feature is numeric or categorical\"\"\"\n",
    "        for i in range(X.shape[1]):\n",
    "            try:\n",
    "                # Try to convert to float to check if numeric\n",
    "                _ = X[:, i].astype(float)\n",
    "                self.feature_types[i] = 'numeric'\n",
    "            except (ValueError, TypeError):\n",
    "                self.feature_types[i] = 'categorical'\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_feats = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        # check the stopping criteria\n",
    "        if (depth >= self.max_depth or n_labels == 1 or n_samples < self.min_samples_split):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        feat_idxs = np.random.choice(n_feats, self.n_features, replace=False)\n",
    "\n",
    "        # find the best split\n",
    "        best_feature, best_thresh = self._best_split(X, y, feat_idxs)\n",
    "        \n",
    "        # If no good split found\n",
    "        if best_feature is None:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        # create child nodes\n",
    "        left_idxs, right_idxs = self._split(X[:, best_feature], best_thresh, best_feature)\n",
    "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n",
    "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n",
    "        return Node(best_feature, best_thresh, left, right)\n",
    "\n",
    "    def _best_split(self, X, y, feat_idxs):\n",
    "        best_gain = -1\n",
    "        split_idx, split_threshold = None, None\n",
    "\n",
    "        for feat_idx in feat_idxs:\n",
    "            X_column = X[:, feat_idx]\n",
    "            thresholds = np.unique(X_column)\n",
    "\n",
    "            for thr in thresholds:\n",
    "                # calculate the information gain\n",
    "                gain = self._information_gain(y, X_column, thr, feat_idx)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feat_idx\n",
    "                    split_threshold = thr\n",
    "\n",
    "        return split_idx, split_threshold\n",
    "\n",
    "    def _information_gain(self, y, X_column, threshold, feat_idx):\n",
    "        # parent entropy\n",
    "        parent_entropy = self._entropy(y)\n",
    "\n",
    "        # create children\n",
    "        left_idxs, right_idxs = self._split(X_column, threshold, feat_idx)\n",
    "\n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "            return 0\n",
    "        \n",
    "        # calculate the weighted avg. entropy of children\n",
    "        n = len(y)\n",
    "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
    "        e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n",
    "        child_entropy = (n_l / n) * e_l + (n_r / n) * e_r\n",
    "\n",
    "        # calculate the IG\n",
    "        information_gain = parent_entropy - child_entropy\n",
    "        return information_gain\n",
    "\n",
    "    def _split(self, X_column, split_thresh, feat_idx):\n",
    "        \"\"\"Split data based on feature type\"\"\"\n",
    "        if self.feature_types.get(feat_idx, 'numeric') == 'numeric':\n",
    "            # Numeric split\n",
    "            try:\n",
    "                X_column_float = X_column.astype(float)\n",
    "                split_thresh_float = float(split_thresh)\n",
    "                left_idxs = np.where(X_column_float <= split_thresh_float)[0]\n",
    "                right_idxs = np.where(X_column_float > split_thresh_float)[0]\n",
    "            except:\n",
    "                # Fallback to categorical if conversion fails\n",
    "                left_idxs = np.where(X_column == split_thresh)[0]\n",
    "                right_idxs = np.where(X_column != split_thresh)[0]\n",
    "        else:\n",
    "            # Categorical split\n",
    "            left_idxs = np.where(X_column == split_thresh)[0]\n",
    "            right_idxs = np.where(X_column != split_thresh)[0]\n",
    "        return left_idxs, right_idxs\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        \"\"\"Calculate entropy that works with any data type\"\"\"\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "        \n",
    "        # Use Counter instead of bincount for mixed data types\n",
    "        counts = Counter(y)\n",
    "        total = len(y)\n",
    "        entropy_val = 0.0\n",
    "        \n",
    "        for count in counts.values():\n",
    "            p = count / total\n",
    "            if p > 0:\n",
    "                entropy_val -= p * np.log(p)\n",
    "                \n",
    "        return entropy_val\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        if len(y) == 0:\n",
    "            return None\n",
    "        counter = Counter(y)\n",
    "        value = counter.most_common(1)[0][0]\n",
    "        return value\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Convert to numpy array for consistent indexing\n",
    "        X = self._convert_to_array(X)\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "\n",
    "        feature_value = x[node.feature]\n",
    "        feature_type = self.feature_types.get(node.feature, 'numeric')\n",
    "        \n",
    "        if feature_type == 'numeric':\n",
    "            # Numeric comparison\n",
    "            try:\n",
    "                feature_value_float = float(feature_value)\n",
    "                threshold_float = float(node.threshold)\n",
    "                if feature_value_float <= threshold_float:\n",
    "                    return self._traverse_tree(x, node.left)\n",
    "                else:\n",
    "                    return self._traverse_tree(x, node.right)\n",
    "            except:\n",
    "                # Fallback to categorical comparison\n",
    "                if str(feature_value) == str(node.threshold):\n",
    "                    return self._traverse_tree(x, node.left)\n",
    "                else:\n",
    "                    return self._traverse_tree(x, node.right)\n",
    "        else:\n",
    "            # Categorical comparison\n",
    "            if str(feature_value) == str(node.threshold):\n",
    "                return self._traverse_tree(x, node.left)\n",
    "            else:\n",
    "                return self._traverse_tree(x, node.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf591ce7-70e7-474b-8454-0cd92cd9e209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Test the code\n",
    "dt_data = pd.read_csv('dt_scratch_data.csv')\n",
    "x = dt_data.iloc[:, :-1]\n",
    "y = dt_data.iloc[:, -1]\n",
    "\n",
    "# If your target variable is categorical, encode it\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "if y.dtype == 'object':\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "dt = DecisionTree(max_depth=3)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "def accuracy(y_test, y_pred):\n",
    "    return np.sum(y_test == y_pred) / len(y_test)\n",
    "\n",
    "acc = accuracy(y_test, y_pred)\n",
    "print(f\"Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc59959c-11e6-4070-8e9a-2e004b635d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "data = datasets.load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=1234\n",
    ")\n",
    "\n",
    "clf = DecisionTree(max_depth=10)\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "def accuracy(y_test, y_pred):\n",
    "    return np.sum(y_test == y_pred) / len(y_test)\n",
    "\n",
    "acc = accuracy(y_test, predictions)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83055c72-baea-4e42-abbd-67ceabaa6852",
   "metadata": {},
   "source": [
    "## CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "43d30995-d6f7-48ef-8ce6-b0a9d86da73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "        \n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split=2, max_depth=100, n_features=None):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.n_features = n_features\n",
    "        self.root = None\n",
    "        self.feature_types = {}  # Store feature types for prediction\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Convert to numpy arrays and handle mixed data types\n",
    "        X = self._convert_to_array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        # Determine feature types\n",
    "        self._determine_feature_types(X)\n",
    "        \n",
    "        self.n_features = X.shape[1] if not self.n_features else min(X.shape[1], self.n_features)\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    def _convert_to_array(self, X):\n",
    "        \"\"\"Convert input to numpy array while preserving data types\"\"\"\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            # Convert each column separately to handle mixed types\n",
    "            arrays = []\n",
    "            for col in X.columns:\n",
    "                col_data = X[col].values\n",
    "                # Try to convert to numeric, if fails keep as object\n",
    "                try:\n",
    "                    arrays.append(pd.to_numeric(col_data).values.reshape(-1, 1))\n",
    "                except:\n",
    "                    arrays.append(col_data.reshape(-1, 1))\n",
    "            return np.hstack(arrays)\n",
    "        else:\n",
    "            return np.array(X)\n",
    "\n",
    "    def _determine_feature_types(self, X):\n",
    "        \"\"\"Determine if each feature is numeric or categorical\"\"\"\n",
    "        for i in range(X.shape[1]):\n",
    "            try:\n",
    "                # Try to convert to float to check if numeric\n",
    "                _ = X[:, i].astype(float)\n",
    "                self.feature_types[i] = 'numeric'\n",
    "            except (ValueError, TypeError):\n",
    "                self.feature_types[i] = 'categorical'\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_feats = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        # check the stopping criteria\n",
    "        if (depth >= self.max_depth or n_labels == 1 or n_samples < self.min_samples_split):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        feat_idxs = np.random.choice(n_feats, self.n_features, replace=False)\n",
    "\n",
    "        # find the best split\n",
    "        best_feature, best_thresh = self._best_split(X, y, feat_idxs)\n",
    "        \n",
    "        # If no good split found\n",
    "        if best_feature is None:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        # create child nodes\n",
    "        left_idxs, right_idxs = self._split(X[:, best_feature], best_thresh, best_feature)\n",
    "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n",
    "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n",
    "        return Node(best_feature, best_thresh, left, right)\n",
    "\n",
    "    def _best_split(self, X, y, feat_idxs):\n",
    "        best_gini = float('inf')  # Changed: minimize Gini instead of maximize Gain\n",
    "        split_idx, split_threshold = None, None\n",
    "\n",
    "        for feat_idx in feat_idxs:\n",
    "            X_column = X[:, feat_idx]\n",
    "            thresholds = np.unique(X_column)\n",
    "\n",
    "            for thr in thresholds:\n",
    "                # calculate the gini impurity\n",
    "                gini = self._gini_impurity(y, X_column, thr, feat_idx)\n",
    "\n",
    "                if gini < best_gini:  # Changed: look for minimum Gini\n",
    "                    best_gini = gini\n",
    "                    split_idx = feat_idx\n",
    "                    split_threshold = thr\n",
    "\n",
    "        return split_idx, split_threshold\n",
    "\n",
    "    def _gini_impurity(self, y, X_column, threshold, feat_idx):\n",
    "        \"\"\"Calculate Gini impurity for a split (CART algorithm)\"\"\"\n",
    "        # create children\n",
    "        left_idxs, right_idxs = self._split(X_column, threshold, feat_idx)\n",
    "\n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "            return float('inf')  # Return worst case if split is invalid\n",
    "        \n",
    "        # calculate the weighted gini impurity of children\n",
    "        n = len(y)\n",
    "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
    "        \n",
    "        # Calculate Gini for left and right children\n",
    "        gini_l = self._gini(y[left_idxs])\n",
    "        gini_r = self._gini(y[right_idxs])\n",
    "        \n",
    "        # Weighted average of children Gini impurities\n",
    "        weighted_gini = (n_l / n) * gini_l + (n_r / n) * gini_r\n",
    "        \n",
    "        return weighted_gini\n",
    "\n",
    "    def _gini(self, y):\n",
    "        \"\"\"Calculate Gini impurity for a node\"\"\"\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "        \n",
    "        # Use Counter to handle any data type\n",
    "        counts = Counter(y)\n",
    "        total = len(y)\n",
    "        gini = 1.0\n",
    "        \n",
    "        for count in counts.values():\n",
    "            p = count / total\n",
    "            gini -= p ** 2\n",
    "            \n",
    "        return gini\n",
    "\n",
    "    def _split(self, X_column, split_thresh, feat_idx):\n",
    "        \"\"\"Split data based on feature type\"\"\"\n",
    "        if self.feature_types.get(feat_idx, 'numeric') == 'numeric':\n",
    "            # Numeric split\n",
    "            try:\n",
    "                X_column_float = X_column.astype(float)\n",
    "                split_thresh_float = float(split_thresh)\n",
    "                left_idxs = np.where(X_column_float <= split_thresh_float)[0]\n",
    "                right_idxs = np.where(X_column_float > split_thresh_float)[0]\n",
    "            except:\n",
    "                # Fallback to categorical if conversion fails\n",
    "                left_idxs = np.where(X_column == split_thresh)[0]\n",
    "                right_idxs = np.where(X_column != split_thresh)[0]\n",
    "        else:\n",
    "            # Categorical split\n",
    "            left_idxs = np.where(X_column == split_thresh)[0]\n",
    "            right_idxs = np.where(X_column != split_thresh)[0]\n",
    "        return left_idxs, right_idxs\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        if len(y) == 0:\n",
    "            return None\n",
    "        counter = Counter(y)\n",
    "        value = counter.most_common(1)[0][0]\n",
    "        return value\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Convert to numpy array for consistent indexing\n",
    "        X = self._convert_to_array(X)\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "\n",
    "        feature_value = x[node.feature]\n",
    "        feature_type = self.feature_types.get(node.feature, 'numeric')\n",
    "        \n",
    "        if feature_type == 'numeric':\n",
    "            # Numeric comparison\n",
    "            try:\n",
    "                feature_value_float = float(feature_value)\n",
    "                threshold_float = float(node.threshold)\n",
    "                if feature_value_float <= threshold_float:\n",
    "                    return self._traverse_tree(x, node.left)\n",
    "                else:\n",
    "                    return self._traverse_tree(x, node.right)\n",
    "            except:\n",
    "                # Fallback to categorical comparison\n",
    "                if str(feature_value) == str(node.threshold):\n",
    "                    return self._traverse_tree(x, node.left)\n",
    "                else:\n",
    "                    return self._traverse_tree(x, node.right)\n",
    "        else:\n",
    "            # Categorical comparison\n",
    "            if str(feature_value) == str(node.threshold):\n",
    "                return self._traverse_tree(x, node.left)\n",
    "            else:\n",
    "                return self._traverse_tree(x, node.right)\n",
    "\n",
    "\n",
    "# Test the code\n",
    "dt_data = pd.read_csv('dt_scratch_data.csv')\n",
    "x = dt_data.iloc[:, :-1]\n",
    "y = dt_data.iloc[:, -1]\n",
    "\n",
    "# If your target variable is categorical, encode it\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "if y.dtype == 'object':\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "dt = DecisionTree(max_depth=3)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "def accuracy(y_test, y_pred):\n",
    "    return np.sum(y_test == y_pred) / len(y_test)\n",
    "\n",
    "acc = accuracy(y_test, y_pred)\n",
    "print(f\"Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "feac8a3a-3c1d-42d4-a706-3bcc315adaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Node class for the tree\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None\n",
    "\n",
    "\n",
    "class DecisionTreeCART:\n",
    "    def __init__(self, min_samples_split=2, n_features=None):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.n_features = n_features\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # convert to numpy\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        self.n_features = X.shape[1] if not self.n_features else min(X.shape[1], self.n_features)\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    def _grow_tree(self, X, y):\n",
    "        n_samples, n_feats = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        # stopping conditions\n",
    "        if (n_labels == 1) or (n_samples < self.min_samples_split):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        # choose random features\n",
    "        feat_idxs = np.random.choice(n_feats, self.n_features, replace=False)\n",
    "\n",
    "        # find the best split\n",
    "        best_feat, best_thresh = self._best_split(X, y, feat_idxs)\n",
    "\n",
    "        if best_feat is None:  # if no split improves\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        # split data\n",
    "        left_idxs, right_idxs = self._split(X[:, best_feat], best_thresh)\n",
    "        left = self._grow_tree(X[left_idxs, :], y[left_idxs])\n",
    "        right = self._grow_tree(X[right_idxs, :], y[right_idxs])\n",
    "        return Node(best_feat, best_thresh, left, right)\n",
    "\n",
    "    def _best_split(self, X, y, feat_idxs):\n",
    "        best_gain = -1\n",
    "        split_idx, split_threshold = None, None\n",
    "\n",
    "        for feat_idx in feat_idxs:\n",
    "            X_column = X[:, feat_idx]\n",
    "            thresholds = np.unique(X_column)\n",
    "\n",
    "            for thr in thresholds:\n",
    "                # calculate gini gain\n",
    "                gain = self._information_gain(y, X_column, thr)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feat_idx\n",
    "                    split_threshold = thr\n",
    "\n",
    "        return split_idx, split_threshold\n",
    "\n",
    "    def _information_gain(self, y, X_column, threshold):\n",
    "        # parent gini\n",
    "        parent_gini = self._gini(y)\n",
    "\n",
    "        # create children\n",
    "        left_idxs, right_idxs = self._split(X_column, threshold)\n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "            return 0\n",
    "\n",
    "        # weighted avg. gini\n",
    "        n = len(y)\n",
    "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
    "        gini_l, gini_r = self._gini(y[left_idxs]), self._gini(y[right_idxs])\n",
    "        child_gini = (n_l / n) * gini_l + (n_r / n) * gini_r\n",
    "\n",
    "        # information gain\n",
    "        ig = parent_gini - child_gini\n",
    "        return ig\n",
    "\n",
    "    def _split(self, X_column, split_thresh):\n",
    "        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
    "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
    "        return left_idxs, right_idxs\n",
    "\n",
    "    def _gini(self, y):\n",
    "        hist = np.bincount(y)\n",
    "        ps = hist / len(y)\n",
    "        return 1 - np.sum(ps ** 2)\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        counter = Counter(y)\n",
    "        return counter.most_common(1)[0][0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Example usage (Iris dataset)\n",
    "# ---------------------------\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "tree = DecisionTreeCART(min_samples_split=2)\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred = tree.predict(X_test)\n",
    "\n",
    "acc = np.sum(y_pred == y_test) / len(y_test)\n",
    "print(\"Accuracy:\", acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b615ac-5670-4a54-8eef-5ca8e5dfedd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
